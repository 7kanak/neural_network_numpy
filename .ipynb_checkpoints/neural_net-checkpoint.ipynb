{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepLearning():\n",
    "    def __init__(self,X,label,learning_rate,lbd=0,batch_size = None):\n",
    "        np.random.seed(2)\n",
    "        np.seterr(all='raise')\n",
    "        self.m = X.shape[1]\n",
    "        self.n = {}\n",
    "        self.prev_layer = X.shape[0]\n",
    "        self.weights = {}\n",
    "        self.bias = {}\n",
    "        self.z = {}\n",
    "        self.a = {}\n",
    "        self.num_layers=0\n",
    "        self.fn = {}\n",
    "        self.da = {}\n",
    "        self.dw = {}\n",
    "        self.db = {}\n",
    "        #self.a[0] = X\n",
    "        self.alpha = learning_rate\n",
    "        self.y = label\n",
    "        self.lbd = lbd\n",
    "        self.keep_prob = {}\n",
    "        self.drop_out = {}\n",
    "        self.X = X\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def add_layer(self,neurons,acti_fn='sigmoid',c=1.0,keep_prob=1.0):\n",
    "        if acti_fn=='relu':\n",
    "            c=2.0\n",
    "        self.weights[self.num_layers+1] = np.random.randn(neurons,self.prev_layer)*np.sqrt(c/self.prev_layer)\n",
    "        #self.weights[self.num_layers+1] = 2*(np.random.rand(neurons,self.prev_layer))-1\n",
    "        self.bias[self.num_layers+1] = np.random.random((neurons,1))\n",
    "        self.num_layers+=1\n",
    "        self.fn[self.num_layers] = acti_fn\n",
    "        self.keep_prob[self.num_layers] = keep_prob\n",
    "        self.prev_layer=neurons\n",
    "    def fit(self,test):\n",
    "        a_tmp = test\n",
    "        for layer in range(1,self.num_layers+1):\n",
    "            z_tmp = np.dot(self.weights[layer],a_tmp) + self.bias[layer]\n",
    "            a_tmp = self.activation_function(z_tmp,self.fn[layer])\n",
    "        return a_tmp\n",
    "    def feed_forward(self,X):\n",
    "        self.a[0] = self.X\n",
    "        for layer in range(1,self.num_layers+1):\n",
    "            self.z[layer] = np.dot(self.weights[layer],self.a[layer-1]) + self.bias[layer]\n",
    "            self.a[layer] = self.activation_function(self.z[layer],self.fn[layer])\n",
    "            if self.keep_prob[layer]!=1:\n",
    "                self.drop_out[layer] = np.random.rand(self.a[layer].shape[0],self.a[layer].shape[1])<self.keep_prob[layer]\n",
    "                self.a[layer] = np.multiply(self.drop_out[layer],self.a[layer])\n",
    "                self.a[layer] /=self.keep_prob[layer]\n",
    "            else:\n",
    "                self.drop_out[layer]=1\n",
    "    \n",
    "    def activation_function(self,x,acti_fn):\n",
    "        if acti_fn == 'tanh':\n",
    "            return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "        if acti_fn == 'relu':\n",
    "            return np.maximum(0.01*x,x)\n",
    "        if acti_fn == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "    def back_pass(self):\n",
    "        #self.da[self.num_layers] = -1*(self.y/self.a[self.num_layers])+(1-self.y)/(1-self.a[self.num_layers])\n",
    "        self.da[self.num_layers] = (self.y-self.a[self.num_layers])/(self.a[self.num_layers]**2-1)\n",
    "        for layer in reversed(range(1,self.num_layers+1)):\n",
    "            tmp = self.da[layer]*self.derivative_fn(self.a[layer],self.fn[layer])\n",
    "            self.dw[layer] = (np.dot(tmp,(self.a[layer-1]).T))/self.m + (self.lbd*self.weights[layer])/(self.m+0.0)\n",
    "            self.db[layer] = (np.sum(tmp,axis=1,keepdims=True))/self.m\n",
    "            self.da[layer-1] = np.dot((self.weights[layer]).T,tmp)\n",
    "            if layer>1:\n",
    "                self.da[layer-1] = self.drop_out[layer-1]*self.da[layer-1]\n",
    "                self.da[layer-1]/=self.keep_prob[layer-1]\n",
    "    \n",
    "    def derivative_fn(self,x,acti_fn):\n",
    "        #x = self.activation_function(x,acti_fn)\n",
    "        if acti_fn == 'tanh':\n",
    "            return 1-x**2\n",
    "        if acti_fn == 'relu':\n",
    "            return np.where(x<=0,0.01,1)\n",
    "        if acti_fn == 'sigmoid':\n",
    "            return x * (1 - x)\n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        for layer in range(1,self.num_layers+1):\n",
    "            self.weights[layer] = self.weights[layer] -self.alpha*self.dw[layer]\n",
    "            self.bias[layer] = self.bias[layer] - self.alpha*self.db[layer]\n",
    "            \n",
    "    def cost_fn(self):\n",
    "        activation=self.a[self.num_layers]\n",
    "        reg_error = 0\n",
    "        for layer in range(1,self.num_layers+1):\n",
    "            reg_error += (np.linalg.norm(self.weights[layer]))**2\n",
    "        reg_error = (reg_error*self.lbd)/(self.m+0.0)\n",
    "        return (-1*np.average(np.log(activation)*self.y + np.log(1-activation)*(1-self.y)))+reg_error,np.sum((activation>0.5).astype(int)==self.y)/(self.m+0.0)\n",
    "    \n",
    "    def train(self):\n",
    "        for i in range(0, 30000):\n",
    "            self.feed_forward(self.X)\n",
    "            self.back_pass()\n",
    "            self.gradient_descent()\n",
    "            print self.cost_fn()\n",
    "            if self.cost_fn()[0]<0.05:\n",
    "                print i,self.cost_fn()\n",
    "                break\n",
    "                \n",
    "    def batch_train(self,batch_size=None):\n",
    "        if batch_size is None:\n",
    "                batch_size = self.m\n",
    "        nb = int(np.ceil((self.m+0.0)/batch_size))\n",
    "        for i in range(0, 30000):\n",
    "            for t in range(0,nb):\n",
    "                X_b = X[:,batch_size*t:batch_size*(t+1)]\n",
    "                self.feed_forward(X_b)\n",
    "                self.back_pass()\n",
    "                self.gradient_descent()\n",
    "                \n",
    "            print self.cost_fn()\n",
    "            if self.cost_fn()[0]<0.1:\n",
    "                print i,self.cost_fn()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mf(a):\n",
    "    return (a[0]+a[1])\n",
    "X = 100*(np.random.random((2,10000)))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = ((np.apply_along_axis(mf,0,X))>110).astype(int).reshape(1,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.034721965268554318, 0.98619999999999997)\n",
      "0 (0.034721965268554318, 0.98619999999999997)\n"
     ]
    }
   ],
   "source": [
    "nn = DeepLearning(X,y,learning_rate=0.5,lbd=0)\n",
    "nn.add_layer(3,acti_fn='relu')\n",
    "nn.add_layer(7,acti_fn='relu')\n",
    "nn.add_layer(5,acti_fn='relu')\n",
    "nn.add_layer(9,acti_fn='relu')\n",
    "nn.add_layer(6,acti_fn='relu')\n",
    "nn.add_layer(9,acti_fn='relu')\n",
    "nn.add_layer(5,acti_fn='relu')\n",
    "nn.add_layer(7,acti_fn='relu')\n",
    "nn.add_layer(3,acti_fn='relu')\n",
    "nn.add_layer(1)\n",
    "st = time.time()\n",
    "nn.batch_train(batch_size=1)\n",
    "et = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: array([[-0.32991595,  0.02455025],\n",
       "        [-0.83048935,  0.0862456 ],\n",
       "        [-2.53331475, -2.66925997]]),\n",
       " 2: array([[ 0.16531026, -0.88124243, -2.38158381],\n",
       "        [ 0.38130625, -0.45239882,  0.95679649],\n",
       "        [ 0.39152829, -0.36028977,  0.99283534],\n",
       "        [ 0.04985939,  0.6486037 ,  0.01002727],\n",
       "        [ 0.04975904, -0.06551715,  0.10666316],\n",
       "        [ 0.0167011 , -0.41464505, -1.69182746],\n",
       "        [ 0.31352675, -0.25677035,  2.03514046]]),\n",
       " 3: array([[ -9.51776182e-01,   4.48240735e-01,  -3.10103018e-01,\n",
       "           2.41432495e-01,   2.08896516e-01,  -1.10898187e+00,\n",
       "           1.78883223e+00],\n",
       "        [  9.79160623e-01,  -4.72851940e-01,   2.39797360e-01,\n",
       "           7.37743122e-02,  -2.93416760e-01,   2.18822516e-01,\n",
       "           8.53596855e-02],\n",
       "        [ -5.85334718e-01,   1.32406502e-02,  -1.18699759e-01,\n",
       "           2.03799779e-01,   2.38865463e-01,  -5.11645686e-01,\n",
       "           5.96245552e-01],\n",
       "        [  4.53869482e-02,  -1.01541976e+00,  -2.28925468e-01,\n",
       "           8.69392065e-04,  -9.33576996e-02,   1.56657989e-01,\n",
       "          -1.11253982e+00],\n",
       "        [  8.42921454e-01,  -7.04629357e-01,  -7.94710787e-01,\n",
       "           2.17562941e-01,   8.18951274e-01,   6.00965232e-02,\n",
       "          -1.88358837e-01]]),\n",
       " 4: array([[-1.13680099, -0.64095593,  0.27589252,  0.02546139, -0.22042615],\n",
       "        [-0.35736399, -0.73158709,  0.45314022,  1.14732792, -0.67242674],\n",
       "        [-0.65876799,  0.36601528,  0.08613936,  0.23757555,  0.47050716],\n",
       "        [ 0.49727265, -1.02181337,  0.28673864, -0.0086195 ,  0.48993226],\n",
       "        [-0.94180477, -0.6797584 , -0.23272372,  0.64011035,  1.15670065],\n",
       "        [ 0.57061327, -0.28648265,  1.00480696,  0.09232834, -0.74068645],\n",
       "        [ 1.9558083 , -0.44723599, -0.5187895 ,  0.51283158,  0.15356966],\n",
       "        [-0.0030707 , -0.90916177,  0.56858219, -0.59241968, -0.44804588],\n",
       "        [-0.18364141, -0.98906646,  0.74693117, -0.01117889,  0.55809829]]),\n",
       " 5: array([[-0.58363119,  0.5477429 , -1.0138842 ,  0.06969125, -0.24652177,\n",
       "         -0.47264975,  0.38688991, -0.0723087 ,  0.56204393],\n",
       "        [-0.69918319,  0.40652773,  0.12543524,  0.17520836, -0.66854032,\n",
       "         -0.10927119, -0.78989341, -0.97358812,  1.62371128],\n",
       "        [ 0.45898638,  0.84957733, -0.35481152,  0.4223274 ,  0.20708617,\n",
       "          0.3503798 ,  0.048326  , -0.0970383 , -0.92694141],\n",
       "        [ 0.2897059 ,  0.21904398,  0.35738668, -0.43108723,  0.54345636,\n",
       "         -0.30962792, -0.73045367,  0.44154994, -0.47511571],\n",
       "        [-0.58535537,  0.05584953,  0.01202975,  0.32863361,  0.24935015,\n",
       "          1.67449926,  0.20540831,  0.73831675, -0.69995198],\n",
       "        [ 0.17393099,  0.37922978, -0.20471878, -0.41408547,  0.16057994,\n",
       "         -0.135945  , -0.28733415,  0.54268853,  0.62959987]]),\n",
       " 6: array([[ 0.16578971, -0.88848504, -0.26388749,  0.23131252,  0.23707809,\n",
       "          0.54763694],\n",
       "        [ 0.38880189, -0.66029399, -0.52520713, -0.7483376 ,  0.69231353,\n",
       "          0.10138325],\n",
       "        [ 0.31432755,  0.57775094, -0.19420788, -0.66510393, -0.27325787,\n",
       "         -0.0901638 ],\n",
       "        [ 0.92744836,  0.46976893, -0.74750084,  0.3414482 ,  0.17184756,\n",
       "         -0.62536967],\n",
       "        [-0.06275687, -1.06640487,  0.19100886, -0.19050641, -0.77869757,\n",
       "         -0.40337342],\n",
       "        [ 0.13343798, -0.50267964,  0.10988437,  0.60451548, -0.61446614,\n",
       "         -0.19615704],\n",
       "        [ 1.24083794,  0.06452517, -0.49199776, -0.04730445, -0.77083576,\n",
       "          0.6394184 ],\n",
       "        [ 0.42472825,  0.73595109,  0.4159938 , -0.10336492,  0.32240612,\n",
       "          0.08832992],\n",
       "        [-0.33686095, -0.48437114, -0.20747335, -1.14408913,  0.99159499,\n",
       "         -0.99863768]]),\n",
       " 7: array([[ -2.38476698e-02,   6.64194217e-01,  -3.70779119e-01,\n",
       "          -1.58467387e-01,  -2.57445415e-01,   3.43990029e-02,\n",
       "           9.82910326e-01,  -7.18729366e-01,  -5.31732008e-01],\n",
       "        [  3.43619012e-01,   8.00057865e-03,  -2.86963579e-01,\n",
       "           1.23873237e-01,  -9.50400028e-01,   8.11264735e-01,\n",
       "          -3.28426732e-01,  -3.35249716e-01,  -7.21190014e-01],\n",
       "        [ -2.77628262e-01,  -3.64189895e-01,  -2.61093904e-03,\n",
       "          -1.12729339e+00,  -4.73659019e-01,  -4.75970874e-01,\n",
       "          -5.59460622e-01,  -3.20220161e-01,  -4.22866284e-01],\n",
       "        [ -5.13841208e-05,   3.24074473e-02,  -6.35435338e-01,\n",
       "           3.54735300e-01,  -1.33182330e-01,   2.69669760e-01,\n",
       "           2.22480415e-01,   8.94858161e-01,   2.04453689e-01],\n",
       "        [ -1.82826437e-02,  -3.84006102e-01,   1.09165100e-01,\n",
       "           2.71955682e-01,  -2.46562543e-03,   8.45191476e-02,\n",
       "          -2.54667369e-01,   6.10778251e-01,  -5.91298065e-01]]),\n",
       " 8: array([[ 1.22075507, -0.68315498, -0.37104532,  0.20239803, -0.74865886],\n",
       "        [-0.30497289,  0.02185456, -0.24582884,  0.11044427,  0.20060813],\n",
       "        [-0.7157924 , -0.43590944,  0.09196573,  0.12716568, -0.29995675],\n",
       "        [ 0.26388783,  0.36298616, -0.41990136,  0.37398928, -1.15311826],\n",
       "        [-1.02434978,  0.26392752, -0.52810045, -0.63298255, -0.20078029],\n",
       "        [-0.40979546,  0.41285554, -0.91771355, -0.50492705,  1.81403447],\n",
       "        [ 0.03155824, -1.47898752,  0.35850164,  0.27412074,  1.07494164]]),\n",
       " 9: array([[ 0.04056028, -0.02205045,  0.66655701,  0.0681127 ,  0.60171548,\n",
       "         -0.42581797,  1.10577473],\n",
       "        [ 0.0390713 ,  0.02703378, -0.05491356,  0.19615172, -0.4483454 ,\n",
       "         -0.36394821, -0.37802643],\n",
       "        [ 0.32412   ,  0.25018679,  0.15575536,  0.77791292,  1.33053746,\n",
       "         -0.40128738, -0.42805348]]),\n",
       " 10: array([[-1.55069867, -0.9306513 , -0.94866766]])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
